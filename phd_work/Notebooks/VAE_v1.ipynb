{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "data_path = '/home/rfit/Telescope_Array/phd_work/data/normed/pr_q4_14yr_e1_0110_excl_sat_F_excl_geo_F.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys ['norm_param', 'test', 'train', 'val']\n",
      "['mean', 'std']\n",
      "['dt_bundle', 'dt_mask', 'dt_params', 'ev_ids', 'ev_starts', 'mc_params', 'recos', 'wfs_flat']\n"
     ]
    }
   ],
   "source": [
    "with h5.File(data_path,'r') as f:\n",
    "    print('keys', list(f.keys()))\n",
    "    train = f['train']\n",
    "    norm_param = f['norm_param']['dt_params']\n",
    "    norm_param_std = norm_param['std'][()]\n",
    "    norm_param_mean = norm_param['mean'][()]\n",
    "    print(list(norm_param.keys()))\n",
    "    print(list(train.keys()))\n",
    "    dt_params = train['dt_params'][()]\n",
    "    ev_starts = train['ev_starts'][()]\n",
    "    val = f['test']\n",
    "    val_dt_mask = val['dt_mask'][()]\n",
    "    val_dt_params = val['dt_params'][()]\n",
    "    val_ev_starts = val['ev_starts'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((499796,), (4498160,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ev_starts.shape, ev_starts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.11465  ,  2.19913  ,  0.0533297,  1.8858643, -4.607    ,\n",
       "        4.362    ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "dt_params[1]*norm_param_std + norm_param_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dt_params (num_dets,6):\n",
    "0. detector x relative to shower core, 1200 m units\n",
    "1. detector y relative to shower core, 1200 m units\n",
    "2. detector z, 1200 m units\n",
    "3. detector signal MIP\n",
    "4. time of the plane front arrival, mks\n",
    "5. time of the waveform relative to the plane front, mks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([        0,        43,        73, ..., 111191147, 111191169,\n",
       "       111191190])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 4.42080e+04, 1.12861e+05, 1.62915e+05,\n",
       "        1.85646e+05, 1.89262e+05, 1.84136e+05, 1.75026e+05, 1.68160e+05,\n",
       "        1.61514e+05, 1.54314e+05, 1.47710e+05, 1.40716e+05, 1.33239e+05,\n",
       "        1.26908e+05, 1.21622e+05, 1.15912e+05, 1.10966e+05, 1.07531e+05,\n",
       "        1.04043e+05, 1.01588e+05, 9.84940e+04, 9.69020e+04, 9.45740e+04,\n",
       "        9.10940e+04, 8.86930e+04, 8.55780e+04, 8.20510e+04, 7.91710e+04,\n",
       "        7.55220e+04, 7.20130e+04, 6.86600e+04, 6.52650e+04, 6.20630e+04,\n",
       "        5.90220e+04, 5.55140e+04, 5.28300e+04, 5.00310e+04, 4.75170e+04,\n",
       "        4.47500e+04, 4.16830e+04, 3.93200e+04, 3.62470e+04, 3.37840e+04,\n",
       "        3.08510e+04, 2.81300e+04, 2.59300e+04, 2.26850e+04, 2.05690e+04,\n",
       "        1.78300e+04, 1.56430e+04, 1.33530e+04, 1.13990e+04, 9.47800e+03,\n",
       "        7.69700e+03, 6.24100e+03, 4.93600e+03, 3.81000e+03, 2.84600e+03,\n",
       "        2.15900e+03, 1.65800e+03, 1.18600e+03, 8.46000e+02, 6.43000e+02,\n",
       "        4.19000e+02, 2.64000e+02, 2.22000e+02, 1.14000e+02, 7.40000e+01,\n",
       "        4.70000e+01, 3.60000e+01, 2.10000e+01, 9.00000e+00, 3.00000e+00,\n",
       "        3.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00]),\n",
       " array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "         22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "         33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "         44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "         55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "         66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "         77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "         88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "         99., 100.]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.diff(ev_starts), bins = 100, range = (0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111191190, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by(data, key = 'plane'):\n",
    "    if key == 'plane':\n",
    "        data = data.sort(5)\n",
    "    return data\n",
    "# sorted_by(dt_params[:43], key = 'plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10,  9, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 22, 21, 19, 20, 23, 24, 25, 26, 27, 28, 29, 30, 32, 31, 33,\n",
       "       34, 35, 36, 37, 38, 40, 41, 39, 42])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(dt_params[:43,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4498160,), (111191190, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_starts.shape, dt_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, ev_start):\n",
    "    st = ev_start[0]\n",
    "    for fn in ev_start[1:]:\n",
    "        data_ev = data[st:fn]\n",
    "        st = fn\n",
    "    \n",
    "data_split(dt_params, ev_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1+cu111'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from https://github.com/Sebastian-Ballesteros/music_tranformer_vae/blob/main/GPTmuseVAE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 256 8 6 128 0.1 30\n",
      "9.92606 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc21 = nn.Linear(32, z_dim)\n",
    "        self.fc22 = nn.Linear(32, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, 64)\n",
    "        self.fc4 = nn.Linear(64, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) \n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        mu, logvar = self.encode(x.view(B * T, C))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z.view(B, T, -1)), mu, logvar\n",
    "\n",
    "    def forward_z(self, x, z_vector, magnitude=1):\n",
    "        B, T, C = x.size()\n",
    "        mu, logvar = self.encode(x.view(B * T, C))\n",
    "\n",
    "        # Manipulate the latent space\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        z += (magnitude * z_vector)\n",
    "\n",
    "        return self.decode(z.view(B, T, -1)), mu, logvar\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, head_size, n_embd, dropout, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size,n_embd, block_size, dropout) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(head_size * n_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout, block_size):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, dropout, block_size)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTmuseVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout, z_dim):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        print(vocab_size, n_embd, n_head, n_layer, block_size, dropout, z_dim)\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, dropout, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.vae = VAE(n_embd, z_dim)\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None ,latent_vector = None, magnitude= None, device = 'cpu'):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "\n",
    "        ## Incorporate vae here VAE (x,z_vector,magnitude)\n",
    "        if latent_vector is not None:\n",
    "            x_vae, mu, logvar = self.vae.forward_z(x, latent_vector, magnitude)\n",
    "\n",
    "        else:\n",
    "            x_vae, mu, logvar = self.vae.forward(x)\n",
    "\n",
    "        logits = self.lm_head(x_vae) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            prediction_loss = None\n",
    "            vae_loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            prediction_loss = F.cross_entropy(logits, targets)\n",
    "            reconstruction_loss =  F.mse_loss(x_vae, x)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            vae_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        return logits, prediction_loss, vae_loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, latent_vector = None, magnitude = None ):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss , loss_vae = self(idx_cond, targets = None, latent_vector = latent_vector, magnitude = magnitude)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx, magnitude\n",
    "    \n",
    "    def sample_latent (self, idx, device='cpu'):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        mu, logvar = self.vae.encode(x)\n",
    "        x = self.vae.reparameterize(mu, logvar)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def main():\n",
    "    # Example usage:\n",
    "    vocab_size = 10000\n",
    "    n_embd = 256\n",
    "    n_head = 8\n",
    "    n_layer = 6\n",
    "    block_size = 128\n",
    "    dropout = 0.1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = GPTmuseVAE(vocab_size, n_embd, n_head, n_layer, block_size, dropout, 30)\n",
    "\n",
    "    m = model.to(device)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 256 8 6 128 0.1 30\n",
      "9.92606 M parameters\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "block_size = 128\n",
    "dropout = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPTmuseVAE(vocab_size, n_embd, n_head, n_layer, block_size, dropout, 30)\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f228a17f3861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_nested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'draw_graph' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model_graph = draw_graph(model(), input_size=(1,128), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchview (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for torchview\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE LSTM(GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self, data, ev_starts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: список тензоров, где каждый тензор имеет форму (seq_len, 5)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.ev_starts = ev_starts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ev_starts)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        st = self.ev_starts[idx]\n",
    "        fn = self.ev_starts[idx + 1]\n",
    "        return torch.tensor(self.data[st:fn])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Кастомная функция для DataLoader, которая заполняет последовательности до максимальной длины в батче.\n",
    "    \"\"\"\n",
    "    # Извлекаем каждую последовательность в батче\n",
    "    sequences = [item for item in batch]\n",
    "    # Заполняем последовательности до одинаковой длины\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=-1.0)  # (batch_size, max_seq_len, 5)\n",
    "    return padded_sequences\n",
    "\n",
    "# Пример данных (можете заменить его своими реальными данными)\n",
    "# Создадим случайные данные с длиной от 5 до 120 для каждой последовательности\n",
    "data = [torch.randn(torch.randint(5, 121, (1,)).item(), 5) for _ in range(100)]  # 100 последовательностей разной длины\n",
    "# Инициализируем датасет и DataLoader\n",
    "dataset = VariableLengthDataset(dt_params, ev_starts)\n",
    "val_dataset = VariableLengthDataset(val_dt_params, val_ev_starts)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Проверка\n",
    "# for batch in train_loader:\n",
    "#     print(\"Batch size:\", batch.size(0))  # размер батча\n",
    "#     print(\"Batch sequence length:\", batch.size(1))  # длина самой длинной последовательности в батче\n",
    "#     print(\"Data shape:\", batch.shape)  # (batch_size, max_seq_len_in_batch, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0:   0%|          | 0/8997 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 9.49 GiB already allocated; 3.44 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6da595511dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/robert_venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-6da595511dfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mrecon_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/robert_venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-6da595511dfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# h_n shape: (1, batch_size, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# убираем первую размерность\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# среднее латентного пространства\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/robert_venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/robert_venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 692\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 9.49 GiB already allocated; 3.44 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)  # h_n shape: (1, batch_size, hidden_dim)\n",
    "        h_n = h_n.squeeze(0)  # убираем первую размерность\n",
    "        mu = self.fc_mu(h_n)  # среднее латентного пространства\n",
    "        log_var = self.fc_logvar(h_n)  # логарифм дисперсии латентного пространства\n",
    "        return mu, log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        h = torch.relu(self.fc(z)).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "        h = h.repeat(1, seq_len, 1)  # Повторяем скрытое состояние для каждого шага времени\n",
    "        lstm_out, _ = self.lstm(h)  # Проходим через LSTM\n",
    "        return self.output_layer(lstm_out)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, latent_dim=16):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon_x = self.decoder(z, seq_len)\n",
    "        return recon_x, mu, log_var\n",
    "\n",
    "# Функция потерь\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    recon_loss = nn.MSELoss()(recon_x, x)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_divergence / x.size(0)\n",
    "\n",
    "# Настройка обучения\n",
    "input_dim = 6\n",
    "hidden_dim = 64\n",
    "latent_dim = 16\n",
    "batch_size = 500\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Цикл обучения (предполагается, что train_loader предоставляет пакеты последовательностей переменной длины)\n",
    "epochs = 20\n",
    "PATH = '../Models_VAE/model1'\n",
    "os.makedirs(PATH, exist_ok = True)\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(train_loader, desc =f\"Epoch {epoch + 1}/{epochs}, Loss: 0.0\")\n",
    "    for x in pbar:  # x должен быть пакетом последовательностей с заполнением\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, log_var = model(x)\n",
    "        loss = vae_loss(recon_x, x, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    loss_mean = []\n",
    "    for x in tqdm(val_loader, desc ='Validation'):  # x должен быть пакетом последовательностей с заполнением\n",
    "        x = x.to(device)\n",
    "        recon_x, mu, log_var = model(x)\n",
    "        loss = vae_loss(recon_x, x, mu, log_var)\n",
    "        loss_mean.append(loss)\n",
    "    torch.save(model.state_dict(), os.path.join(PATH, f'epoch_{epoch}'))\n",
    "    print(f'Epoch {epoch + 1}, Loss: {np.array(loss_mean).mean()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/articles/486358/ - Trnsformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
