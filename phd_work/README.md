 # TASKS

Энкодер:
1. Добавить слой погружения погружения как в трансформерах (не принципиально, но потом пригодиться).
2. Вероятно, что для слой погружения лучше сработают свертки, чем полносвязные слои (проверять совсем потом). 

Функция потерь:
1. Ввести маску и умножеть на неё функцию потерь по реконструкции, чтобы факторизоваться от задачи нахождения конца последовательности.
2. Подумать, почему три разные модели сошлись к одному и тому же на обучении. Лимитировал явно какой-то общий для них слой. Иначе непонятно почему они настолько синхронно ходили вверх-вниз.

Декодер.
1. Судя по коду, у декодера размерность внутренних представлений [32, 64, 128]. Это так, в такой последовательности? Тогда не удивительно, что фитируется полиномом малой степени. Должно быть наоборот. В такой вариации, у вас размерность латентного пространство порядка 32, и все остальные lstm слои не помогут. 
2. Сделать полносвязный слой, подготавливающий начальное состояние lstm (c_0 и h_0) из вектора в латентном пространстве. Может заметно улучшить генерацию.
3. Ввести токен начала последовательности. В вашей реализации он может быть избыточным, или надо вводить дополнительный "начальный" элемент.
4. Сделать альтернативный вариант декодера, который генерирует один элемент за раз, также используя своё предыдущее предсказание (см. также пример в следующем сообщении).

Общее:
1. Как мы увидели, много слоев lstm пока не нужны. Интереснее увеличить их внутренний размер и размерность латентного пространства.
2. У вас в конфиге стоит input_dim: 6. Размерность входных параметров 5 или все таки 6? С неправильной размерностью он бы не собирался.
3. Разобраться, как происходит перевод предложений в NLP с помощью lstm. Как используется токен начала предложения и как именно происходит генерация.